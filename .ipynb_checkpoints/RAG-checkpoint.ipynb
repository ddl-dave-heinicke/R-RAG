{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e96ba-1b6b-403b-94b7-8b5fbc112c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install pinecone-client==2.2.4 #restart the kernel after executing this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3072f3ad-74cf-4596-b37e-755ca8671cf2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy==2.0.1 in /opt/conda/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from SQLAlchemy==2.0.1) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy==2.0.1) (3.0.3)\n",
      "Requirement already satisfied: langchain-openai==0.0.5 in /opt/conda/lib/python3.9/site-packages (0.0.5)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /opt/conda/lib/python3.9/site-packages (from langchain-openai==0.0.5) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain-openai==0.0.5) (1.23.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from langchain-openai==0.0.5) (1.28.1)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain-openai==0.0.5) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (1.33)\n",
      "Requirement already satisfied: langsmith<0.0.88,>=0.0.87 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (0.0.87)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (0.23.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ubuntu/.local/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.9/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai==0.0.5) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (2023.11.17)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /opt/conda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (0.16.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (1.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/.local/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain-openai==0.0.5) (1.26.18)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.9/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.0.5) (0.14.0)\n",
      "Requirement already satisfied: langchain-anthropic in /opt/conda/lib/python3.9/site-packages (0.1.11)\n",
      "Requirement already satisfied: anthropic<1,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from langchain-anthropic) (0.25.8)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from langchain-anthropic) (0.7.1)\n",
      "Collecting langchain-core<0.2.0,>=0.1.43 (from langchain-anthropic)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (0.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (1.10.14)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (1.3.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ubuntu/.local/lib/python3.9/site-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.43->langchain-anthropic)\n",
      "  Using cached langsmith-0.1.56-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.23.0->langchain-anthropic) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.23.0->langchain-anthropic) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic) (2023.11.17)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /opt/conda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic) (0.16.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic) (1.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/.local/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.9/site-packages (from tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (0.23.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.9/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (2023.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.43->langchain-anthropic) (1.26.18)\n",
      "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Using cached langsmith-0.1.56-py3-none-any.whl (120 kB)\n",
      "Installing collected packages: langsmith, langchain-core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.23\n",
      "    Uninstalling langchain-core-0.1.23:\n",
      "      Successfully uninstalled langchain-core-0.1.23\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.4 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.56 which is incompatible.\n",
      "langchain-community 0.0.16 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.56 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.1.52 langsmith-0.1.56\n",
      "Requirement already satisfied: ragatouille in /opt/conda/lib/python3.9/site-packages (0.0.8.post2)\n",
      "Requirement already satisfied: colbert-ai==0.2.19 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (0.2.19)\n",
      "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.4 in /home/ubuntu/.local/lib/python3.9/site-packages (from ragatouille) (1.7.4)\n",
      "Requirement already satisfied: fast-pytorch-kmeans==0.2.0.1 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (0.2.0.1)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from ragatouille) (0.1.4)\n",
      "Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (0.1.52)\n",
      "Requirement already satisfied: llama-index>=0.7 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (0.10.36)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (1.16.0)\n",
      "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (2.7.0)\n",
      "Requirement already satisfied: srsly==2.4.8 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (2.4.8)\n",
      "Requirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (2.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (4.40.2)\n",
      "Requirement already satisfied: voyager<3.0.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from ragatouille) (2.0.6)\n",
      "Requirement already satisfied: bitarray in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (2.9.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (2.19.1)\n",
      "Requirement already satisfied: flask in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (2.2.5)\n",
      "Requirement already satisfied: git-python in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (1.11.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (4.66.4)\n",
      "Requirement already satisfied: ujson in /opt/conda/lib/python3.9/site-packages (from colbert-ai==0.2.19->ragatouille) (5.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.23.3)\n",
      "Requirement already satisfied: pynvml in /opt/conda/lib/python3.9/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (11.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /opt/conda/lib/python3.9/site-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.5.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.16)\n",
      "Collecting langsmith<0.1,>=0.0.83 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_core<0.2.0,>=0.1.4 (from ragatouille)\n",
      "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.9/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (4.2.0)\n",
      "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1,>=0.0.83 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
      "  Using cached langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.9/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (23.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.2.4)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.10.36)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.9)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.18)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.5)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from llama-index>=0.7->ragatouille) (0.1.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.9/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13->ragatouille) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->ragatouille) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.20.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/.local/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.28.1)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (0.23.3)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (3.8.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.5.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/ubuntu/.local/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (0.5.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/conda/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>=0.7->ragatouille) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.9/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.9/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/lib/python3.9/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.9/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.13->ragatouille) (2.1.4)\n",
      "Requirement already satisfied: gitpython in /opt/conda/lib/python3.9/site-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.41)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask->colbert-ai==0.2.19->ragatouille) (3.17.0)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /opt/conda/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (0.16.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /opt/conda/lib/python3.9/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index>=0.7->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (1.0.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (2023.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.9/site-packages (from httpcore<0.17.0,>=0.15.0->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index>=0.7->ragatouille) (1.16.0)\n",
      "Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "Installing collected packages: langsmith, langchain_core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.56\n",
      "    Uninstalling langsmith-0.1.56:\n",
      "      Successfully uninstalled langsmith-0.1.56\n",
      "  Attempting uninstall: langchain_core\n",
      "    Found existing installation: langchain-core 0.1.52\n",
      "    Uninstalling langchain-core-0.1.52:\n",
      "      Successfully uninstalled langchain-core-0.1.52\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-anthropic 0.1.11 requires langchain-core<0.2.0,>=0.1.43, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain_core-0.1.23 langsmith-0.0.87\n",
      "Requirement already satisfied: langchain[all] in /home/ubuntu/.local/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (2.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain[all]) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain[all]) (0.5.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain[all]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /home/ubuntu/.local/lib/python3.9/site-packages (from langchain[all]) (0.0.16)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (0.1.23)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (1.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain[all]) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[all]) (3.20.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[all]) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[all]) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ubuntu/.local/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain[all]) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain[all]) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain[all]) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from pydantic<3,>=1->langchain[all]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain[all]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain[all]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain[all]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain[all]) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain[all]) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain[all]) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain[all]) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[all]) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SQLAlchemy==2.0.1\n",
    "! pip install langchain-openai==0.0.5\n",
    "! pip install langchain-anthropic\n",
    "! pip install ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47aaeca-4720-4c78-aee1-51dde0d2e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from langchain.schema import HumanMessage, SystemMessage\n",
    "# #from langchain.chains import ConversationChain, HypotheticalDocumentEmbedder, LLMChain\n",
    "# #from langchain_community.vectorstores import Qdrant\n",
    "# #from langchain.memory import ConversationSummaryMemory\n",
    "# #from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# #from langchain_community.chat_models import ChatMlflow\n",
    "\n",
    "# #from langchain_openai import ChatOpenAI\n",
    "# #from langchain_anthropic import ChatAnthropic\n",
    "# from langchain import PromptTemplate\n",
    "\n",
    "# from domino_data.vectordb import DominoPineconeConfiguration\n",
    "# # from ragatouille import RAGPretrainedodel\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5dd7a8-e83a-4964-9c40-1e74cca7726d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chains import ConversationChain, HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.chat_models import ChatMlflow\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool, Tool\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from domino_data.vectordb import DominoPineconeConfiguration\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "import os\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6c38b1-47e2-451b-9e39-ffde7db7cfab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/{}/model_cache/'.format(os.environ['DOMINO_PROJECT_NAME'])\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=embedding_model_name,\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6d845c-bb8c-4ec6-b3de-61c0f8768bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Pinecone index\n",
    "datasource_name = \"pinecone-domino-support\"\n",
    "conf = DominoPineconeConfiguration(datasource=datasource_name)\n",
    "api_key = os.environ.get(\"DOMINO_VECTOR_DB_METADATA\", datasource_name)\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=api_key,\n",
    "    environment=\"domino\",\n",
    "    openapi_config=conf\n",
    ")\n",
    "\n",
    "index = pinecone.Index(\"domino-support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925c2b3e-ee1c-48be-817f-e736e4fe2ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatMlflow(\n",
    "    target_uri=os.environ[\"DOMINO_MLFLOW_DEPLOYMENTS\"],\n",
    "    endpoint=\"Chat-gpt35turbo-se\",\n",
    ")\n",
    "\n",
    "conversation_openai = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=ConversationSummaryMemory(llm=chat),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# conversation_anthropic = ConversationChain(\n",
    "#         llm=ChatAnthropic(model='claude-2.1'),\n",
    "#         memory=ConversationSummaryMemory(llm=ChatAnthropic(model='claude-2.1')),\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0061e75f-b198-43d9-916b-32cf0231edb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup HyDE\n",
    "\n",
    "hyde_prompt_template = \"\"\"You are a virtual assistant for Rakuten and your task is to answer questions related to Rakuten which includes general information about Rakuten\n",
    "\"Please answer the user's question below \\n \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "hyde_prompt = PromptTemplate(input_variables=[\"question\"], template=hyde_prompt_template)\n",
    "hyde_llm_chain = LLMChain(llm=chat, prompt=hyde_prompt)\n",
    "\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=hyde_llm_chain, base_embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316f8710-fc11-4444-9f8c-235141299e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66180cfdd2704d809cb24a398a4b15a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ab7d616dda425db1d9d8f9c390e923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a421071bc164658ad17c1defe300eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98efecb4cf5437c9620a16f3db28107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5df95bb14cd41f485accfe56951a185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dffffd97f0e4c618b94bc00c0f27d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de3f8b097594c028391c4a5404a7b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 11, 15:08:37] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get relevant docs through vector DB\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Number of texts to match (may be less if no suitable match)\n",
    "NUM_TEXT_MATCHES = 5\n",
    "\n",
    "# Number of texts to return from reranking\n",
    "NUM_RERANKING_MATCHES = 3\n",
    "\n",
    "# Create prompt\n",
    "template = \"\"\" You are a virtual assistant for Rakuten and your task is to answer questions related to Rakuten which includes general information about Rakuten.\n",
    "\n",
    "                Respond in the style of a polite helpful assistant and do not allude that you have looked up the context.\n",
    "\n",
    "                Do not hallucinate. If you don't find an answer, you can point user to the official website here: https://www.rakuten.com/help . \n",
    "\n",
    "                In your response, include the following url links at the end of your response {url_links} and any other relevant URL links that you refered.\n",
    "\n",
    "                Also, at the end of your response, ask if your response was helpful\". \n",
    "\n",
    "                Here is some relevant context: {context}\"\"\"\n",
    "\n",
    "# Load the reranking model\n",
    "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Get relevant docs through vector DB\n",
    "def get_relevant_docs(user_input, num_matches=NUM_TEXT_MATCHES, use_hyde=True):\n",
    "   \n",
    "    if use_hyde:\n",
    "        embedded_query = hyde_embeddings.embed_query(user_input)\n",
    "    else:\n",
    "        embedded_query = embeddings.embed_query(user_input)\n",
    "    \n",
    "    relevant_docs = index.query(\n",
    "        namespace='domino-help',\n",
    "        vector=embedded_query,\n",
    "        top_k=num_matches,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    matches = relevant_docs[\"matches\"]\n",
    "    filtered_matches = [match for match in matches if match['score'] >= SIMILARITY_THRESHOLD]\n",
    "    relevant_docs[\"matches\"] = filtered_matches\n",
    "\n",
    "    return relevant_docs\n",
    "\n",
    " \n",
    "def build_system_prompt(user_input, rerank=True, use_hyde=True):\n",
    "    \n",
    "    try:\n",
    "        relevant_docs = get_relevant_docs(user_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get relevant documents: {e}\")\n",
    "        return \"\", \"Failed to get relevant documents\"\n",
    "    \n",
    "    actual_num_matches = len(relevant_docs[\"matches\"])\n",
    "    if actual_num_matches == 0:\n",
    "        print(\"No matches found in relevant documents.\")\n",
    "        return \"\", \"No matches found in relevant documents\"\n",
    "    \n",
    "    urls = set([relevant_docs[\"matches\"][i][\"metadata\"][\"source\"] for i in range(actual_num_matches)])\n",
    "    contexts = [relevant_docs[\"matches\"][i][\"metadata\"][\"text\"] for i in range(actual_num_matches)]\n",
    "    \n",
    "    if rerank and actual_num_matches >= NUM_RERANKING_MATCHES:\n",
    "        try:\n",
    "            docs = colbert.rerank(query=user_input, documents=contexts, k=NUM_RERANKING_MATCHES)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to rerank documents: {e}\")\n",
    "            return \"\", \"Failed to rerank documents\"\n",
    "        \n",
    "        try:\n",
    "            result_indices = [docs[i][\"result_index\"] for i in range(NUM_RERANKING_MATCHES)]\n",
    "        except (IndexError, KeyError) as e:\n",
    "                print(f\"Invalid result indices: {e}\")\n",
    "                return \"\", \"Invalid result indices\"\n",
    "        try:    \n",
    "            contexts = [contexts[index] for index in result_indices]\n",
    "            urls = [list(urls)[index] for index in result_indices]\n",
    "        except IndexError as e:\n",
    "            print(f\"Indexing error: {e}\")\n",
    "            return \"\", \"Indexing error\"\n",
    "    \n",
    "    # Create prompt\n",
    "    template = \"\"\" You are a virtual assistant for Rakuten and your task is to answer questions related to Rakuten which includes general information about Rakuten.\n",
    "\n",
    "                Respond in the style of a polite helpful assistant and do not allude that you have looked up the context.\n",
    "\n",
    "                Do not hallucinate. If you don't find an answer, you can point user to the official website here: https://www.rakuten.com/help . \n",
    "\n",
    "                In your response, include the following url links at the end of your response {url_links} and any other relevant URL links that you refered.\n",
    "\n",
    "                Also, at the end of your response, ask if your response was helpful\". \n",
    "\n",
    "                Here is some relevant context: {context}\"\"\"\n",
    " \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"url_links\", \"context\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    system_prompt = prompt_template.format( url_links=urls, context=contexts)\n",
    " \n",
    "    return system_prompt, contexts\n",
    " \n",
    "# Query the Open AI Model\n",
    "def queryAIModel(user_input, llm_name=\"openai\", use_hyde=True):\n",
    "\n",
    "    if not user_input:\n",
    "        return \"Please provide an input\"\n",
    "    \n",
    "    system_prompt = build_system_prompt(user_input)            \n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=system_prompt\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=user_input\n",
    "        ),\n",
    "    ]\n",
    "    if llm_name.lower() == \"openai\":\n",
    "        output = conversation_openai.predict(input=messages)\n",
    "    else:\n",
    "        output = conversation_anthropic.predict(input=messages)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854478ef-aaf8-41a7-b460-9cbdb6ff2ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : What options do I have for adding code packages to my executions?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get relevant documents: 404 Client Error: Not Found for url: http://127.0.0.1:8767/endpoints/chat-gpt35turbo-sm/invocations. Response text: {\"detail\":\"Not Found\"}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://127.0.0.1:8767/endpoints/chat-gpt35turbo-sm/invocations. Response text: {\"detail\":\"Not Found\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/utils/request_utils.py:63\u001b[0m, in \u001b[0;36maugmented_raise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://127.0.0.1:8767/endpoints/chat-gpt35turbo-sm/invocations",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ask a question ; uncomment to test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# What options do I have for adding code packages to my executions?\u001b[39;00m\n\u001b[1;32m      5\u001b[0m user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide your question here :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqueryAIModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m result\n",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m, in \u001b[0;36mqueryAIModel\u001b[0;34m(user_input, llm_name, use_hyde)\u001b[0m\n\u001b[1;32m    114\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    115\u001b[0m     SystemMessage(\n\u001b[1;32m    116\u001b[0m         content\u001b[38;5;241m=\u001b[39msystem_prompt\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     ),\n\u001b[1;32m    121\u001b[0m ]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_name\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mconversation_openai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     output \u001b[38;5;241m=\u001b[39m conversation_anthropic\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    151\u001b[0m     inputs,\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_community/chat_models/mlflow.py:122\u001b[0m, in \u001b[0;36mChatMlflow._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens\n\u001b[0;32m--> 122\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatMlflow\u001b[38;5;241m.\u001b[39m_create_chat_result(resp)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/deployments/mlflow/__init__.py:305\u001b[0m, in \u001b[0;36mMlflowDeploymentClient.predict\u001b[0;34m(self, deployment_name, inputs, endpoint)\u001b[0m\n\u001b[1;32m    301\u001b[0m query_route \u001b[38;5;241m=\u001b[39m join_paths(\n\u001b[1;32m    302\u001b[0m     MLFLOW_DEPLOYMENTS_ENDPOINTS_BASE, endpoint, MLFLOW_DEPLOYMENTS_QUERY_SUFFIX\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_route\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLFLOW_DEPLOYMENT_PREDICT_TIMEOUT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39m__cause__, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/deployments/mlflow/__init__.py:141\u001b[0m, in \u001b[0;36mMlflowDeploymentClient._call_endpoint\u001b[0;34m(self, method, route, json_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[1;32m    132\u001b[0m response \u001b[38;5;241m=\u001b[39m http_request(\n\u001b[1;32m    133\u001b[0m     host_creds\u001b[38;5;241m=\u001b[39mget_default_host_creds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_uri),\n\u001b[1;32m    134\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mroute,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_kwargs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[0;32m--> 141\u001b[0m \u001b[43maugmented_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/utils/request_utils.py:66\u001b[0m, in \u001b[0;36maugmented_raise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Response text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://127.0.0.1:8767/endpoints/chat-gpt35turbo-sm/invocations. Response text: {\"detail\":\"Not Found\"}"
     ]
    }
   ],
   "source": [
    "# Ask a question ; uncomment to test\n",
    "\n",
    "# What options do I have for adding code packages to my executions?\n",
    "\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = queryAIModel(user_question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe57134-b2b2-4e7a-9110-356f9a2f7650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
