{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6441b8bf-6205-4464-a97c-303ea86203f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client==2.2.4 in /opt/conda/lib/python3.9/site-packages (2.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ubuntu/.local/lib/python3.9/site-packages (from pinecone-client==2.2.4) (4.9.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (1.26.18)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.9/site-packages (from pinecone-client==2.2.4) (1.23.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.4) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client==2.2.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client==2.2.4) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install pinecone-client==2.2.4 #restart the kernel after executing this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f30f5cf-d1ce-47c9-98c4-b8d7a957c244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What options do I have for adding code packages to my executions?\n",
    "%run RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b85cb4f5-64af-4fef-bdf5-fee0fc30b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "from mlflow.metrics.genai import faithfulness, relevance, EvaluationExample\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f347736-72ee-4486-9843-9504ca0efd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a data frame with e-commerce related questions\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"What options do I have for adding code packages to my executions?\",\n",
    "            \"Does Domino support Project environment variables?\",\n",
    "            \"Does Domino have its own managed storage?\",\n",
    "            \"Can I attach an external data volumne?\",\n",
    "            \"Which vector databases does Domino currently support?\",\n",
    "            \"Does Domino have a Feature Store?\"\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76c1ace-72c7-443a-a8d7-85e1e91134c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create good and bad examples for faithfulness in the context of support questions\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"Does Domino have its own managed storage?\",\n",
    "        output=\"Domino Datasets provides high-performance, versioned, and structured filesystem storage in Domino.\",\n",
    "        score=2,\n",
    "        justification=\"The output provides a partially correct answer but misses important context about the the advantages of using Domino Datasets for reproducability\",\n",
    "        grading_context={\n",
    "            \"context\": \" You can use Datasets to build multiple curated collections of data in one Project and share them with your collaborators to use in their Projects. Likewise, you can mount Datasets from other Projects in your own Project if they are shared with you.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"Which vector databases does Domino currently support?\",\n",
    "        output=\"Domino currently supports Pinecone and QDrant\",\n",
    "        score=5,\n",
    "        justification=\"The output accurately reflects the databases currently supported, but does not indicate our plan to add more connectors\",\n",
    "        grading_context={\n",
    "            \"context\": \"Domino is in the process of adding more vector database connectors\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "# Uncomment below if you don't want to use an AI g/w endpoint\n",
    "# faithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\n",
    "faithfulness_metric = faithfulness(model=\"endpoints:/chat-gpt4-ja\", examples=faithfulness_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af380252-7a85-46a8-a8d9-930b140df189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create good and bad examples for relevance in the context of e-commerce questions\n",
    "relevance_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"Why can't I see the hardware tier I was expecting to see in my project?\",\n",
    "        output=\"Root cause: The hardware tiers you are looking for may have restricted access.\",\n",
    "        score=2,\n",
    "        justification=\"The output provides general information, but does not provide a solution. For example, Your local Domino admin team will often restrict access to larger GPU/CPU tiers in order to contain costs on those tiers.\",\n",
    "        grading_context={\n",
    "            \"context\": \"If you don't see hardware tiers you are looking for, reach out to your local admin team for access to those tiers.  Access to hardware tiers in Domino is managed at the organization level.  So you will likely just need to be added to the appropriate Domino org. with access to that hardware tier by your local admin team.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What does Suggested and Popular Projects mean?\",\n",
    "        output=\"The top popular projects are calculated based on how many jobs are getting executed, the number of tags, and the number of collaborators with specific weightings.\",\n",
    "        score=5,\n",
    "        justification=\"The output is highly relevant to the question, providing a clear and concise explanation on exactly how Projects are sugested\",\n",
    "        grading_context={\n",
    "            \"context\": \"The top popular projects are calculated based on how many jobs are getting executed, the number of tags, and the number of collaborators with specific weightings. In Domino's config (this config is not exposed to users), you can add weights to each value (tags, collaborators, job count).\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "# Uncomment below if you don't want to use an AI g/w endpoint\n",
    "# relevance_metric = relevance(model=\"openai:/gpt-4\", examples=relevance_examples)\n",
    "relevance_metric = relevance(model=\"endpoints:/chat-gpt4-ja\", examples=relevance_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779ab32-016b-46c8-abab-2e1c8080de30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that returns the response from the RAG for the evaluation dataset\n",
    "def model(input_df):\n",
    "    answer = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        system_prompt, contexts = build_system_prompt(row[\"questions\"], use_hyde=False)            \n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=system_prompt\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=row[\"questions\"]\n",
    "            ),\n",
    "        ]\n",
    "        response = qa.predict(input=messages)\n",
    "       \n",
    "        answer.append({\"result\":qa.predict(input=messages),\n",
    "                      \"source_documents\":contexts})\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a4ce5a4-adb0-433b-b5cf-e383fcc5389e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name=OpenAI_BAAI/bge-small-en_run\n",
      "model : OpenAI\n",
      "embedding : BAAI/bge-small-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/11 16:18:03 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/05/11 16:18:03 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/11 16:19:01 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9040c654b53a428a83edeced63a8a9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3715: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64022cae529c44f090b03122c414c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n",
      "2024/05/11 16:19:01 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'exact_match' because it returned None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692ad0d1f8e7495aa25c7642974045b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3715: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07924e1af104253a27566abf51c8864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets run the evaluation for the llm-embedding model combinations\n",
    "\n",
    "qa = None\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "# llms = ('OpenAI', 'Anthropic')\n",
    "llms = (['OpenAI'])\n",
    "\n",
    "# Iterate through each combination and execute the evaluations\n",
    "for llm_name in llms:\n",
    "    run_name = f\"{llm_name}_{embedding_model_name}_run\"\n",
    "    print(f'run_name={run_name}')\n",
    "    # Log parameters\n",
    "    print(f\"model : {llm_name}\")\n",
    "    print(f\"embedding : {embedding_model_name}\")\n",
    "    if \"OpenAI\" in llm_name:\n",
    "        qa = conversation_openai\n",
    "    elif \"Anthropic\" in llm_name:\n",
    "        qa= conversation_anthropic\n",
    "    # Run the evaluation\n",
    "    results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    model_type=\"question-answering\",\n",
    "    evaluators=\"default\",\n",
    "    predictions=\"result\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    metrics_series = pd.Series(results.metrics, name=f'{llm_name}_{embedding_model_name}')\n",
    "    metrics_df = pd.DataFrame([metrics_series])\n",
    "    df_metrics = pd.concat([df_metrics, metrics_df], ignore_index=True)\n",
    "    \n",
    "df_metrics = df_metrics.T\n",
    "df_metrics.columns = llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcc29e5e-d864-48d3-a617-1b851008007c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpenAI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latency/mean</th>\n",
       "      <td>9.675278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency/variance</th>\n",
       "      <td>1.586657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latency/p90</th>\n",
       "      <td>10.998815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness/v1/mean</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness/v1/variance</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance/v1/mean</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance/v1/variance</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             OpenAI\n",
       "latency/mean               9.675278\n",
       "latency/variance           1.586657\n",
       "latency/p90               10.998815\n",
       "faithfulness/v1/mean            NaN\n",
       "faithfulness/v1/variance        NaN\n",
       "relevance/v1/mean               NaN\n",
       "relevance/v1/variance           NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caf6f66c-db1f-48d4-aeb9-ee620af95f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets now log these metrics in Domino\n",
    "# Define the experiment name\n",
    "experiment_name = 'default-project-6602e00c6cd93c572ea55308'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "for column in df_metrics:\n",
    "    with mlflow.start_run(run_name=column):\n",
    "        for metric_name, metric_value in df_metrics[column].items():\n",
    "            # Log the metric\n",
    "            mlflow.log_metric(metric_name, metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a122197-bb8b-4de3-bf9f-72f0f30ab575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Looks good lets push the prompt to a prompt hub\n",
    "# set LANGCHAIN_HUB_API_KEY in an env variable\n",
    "# hub.push(\"subirmansukhani/rakuten-qa-rag\", ChatPromptTemplate.from_template(template), new_repo_is_public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f5167-f223-460c-8314-185bd8d33bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets take a look at the prompt hub\n",
    "\n",
    "# from IPython.display import Javascript, display\n",
    "\n",
    "# # Define the URL you want to open\n",
    "# url = 'https://smith.langchain.com/hub/my-prompts?organizationId=6ac11f6f-c332-4bac-b45b-28a8a96410b4'\n",
    "\n",
    "# # JavaScript code to open a new tab with the specified URL and display it in the cell's output area\n",
    "# js_code = f'''\n",
    "# var newWindow = window.open(\"{url}\");\n",
    "# element.append(newWindow.document.body);\n",
    "# '''\n",
    "\n",
    "# # Display the JavaScript output in the cell's output area\n",
    "# display(Javascript(js_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b37a47-500b-4c8b-a4dd-ec874fae410c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
